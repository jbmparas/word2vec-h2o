{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word2vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's understand what is word2vec ?\n",
    "word2vec is `word embedding` wherein similarity comes from `neighbourhood` words.\n",
    "### But what is embedding ?\n",
    "<img src='img\\embedding.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](img\\w2v.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What are different attributes of a `word2vec model` ?\n",
    "- `Input layer` : Accepts one-hot encoded words\n",
    "- `Hidden layer` : Less number of neurons as compared to input layer\n",
    "- `Output layer` -\n",
    "    - Exactly same number of neurons as input layer\n",
    "    - Here we use `softmax function` as activation function\n",
    "    - `Cross-entropy loss` is used to measure error at a softmax layer\n",
    "    - Once we get error, we use gradient descent optimiser to head to direction of `local/global minima`\n",
    "    - After several rounds of optimisation we extract the matrix which is vectors coressponding to each word\n",
    "<img src= \"img\\model_structure_w2v.png\" style=\"width: 620px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's understand what happens inside a **word2vec** model?\n",
    "**Before getting into how does a word2vec model work let's understand what is n-gram?**\n",
    "- `n-gram :` In very simple language n gram means length number of combinations for example -\n",
    "In the sentence - *Kite flies high in the sky*\n",
    "<br>\n",
    "\n",
    "`2-grams` can be represented as below :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Kite', 'flies'),\n",
       " ('flies', 'high'),\n",
       " ('high', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'sky')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import skipgrams, ngrams\n",
    "sent = \"Kite flies high in the sky\".split()\n",
    "list(ngrams(sent, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3-grams` can be represented as below :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Kite', 'flies', 'high'),\n",
       " ('flies', 'high', 'in'),\n",
       " ('high', 'in', 'the'),\n",
       " ('in', 'the', 'sky')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"Kite flies high in the sky\".split()\n",
    "list(ngrams(sent, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarly we can make n-grams for different values of n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'am', 'making'),\n",
       " ('I', 'am', 'it'),\n",
       " ('I', 'am', 'work'),\n",
       " ('I', 'am', 'with'),\n",
       " ('I', 'am', 'hardwork'),\n",
       " ('I', 'making', 'it'),\n",
       " ('I', 'making', 'work'),\n",
       " ('I', 'making', 'with'),\n",
       " ('I', 'making', 'hardwork'),\n",
       " ('I', 'it', 'work'),\n",
       " ('I', 'it', 'with'),\n",
       " ('I', 'it', 'hardwork'),\n",
       " ('I', 'work', 'with'),\n",
       " ('I', 'work', 'hardwork'),\n",
       " ('I', 'with', 'hardwork'),\n",
       " ('am', 'making', 'it'),\n",
       " ('am', 'making', 'work'),\n",
       " ('am', 'making', 'with'),\n",
       " ('am', 'making', 'hardwork'),\n",
       " ('am', 'it', 'work')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"I am making it work with hardwork\".split()\n",
    "list(skipgrams(sent, 3, 4))[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's understand what does k-skip n-gram mean?\n",
    "Now using `3 grams` we will make **4-skip 3-grams:-**\n",
    "Here we can see \n",
    "- 0 skip gram : ('I', 'am', 'making')\n",
    "- 1 skip gram : ('I', 'am', 'it')\n",
    "- 2 skip gram : ('I', 'am', 'work')\n",
    "- 3 skip gram : ('I', 'am', 'with')\n",
    "- 4 skip gram : ('I', 'am', 'hardwork')\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Here we can see for 4th skip gram **am** is 0 skip since **am** occurs just after **I** after that **hardwork** occurs at 4th index after **am** if we start counting from **0**\n",
    "\n",
    "#### Similarly it will skip 2and gram now and it will look like this :-\n",
    "\n",
    "<br>\n",
    "\n",
    "- 0 skip gram : ('I', 'making', 'it')\n",
    "- 1 skip gram : ('I', 'making', 'work')\n",
    "- 2 skip gram : ('I', 'making', 'with')\n",
    "- 3 skip gram : ('I', 'making', 'hardwork')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How a word2vec model works? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have built k-skip n-gram we have to use those grams in the word2vec model.\n",
    "<br>\n",
    "But how are we going to do that. That is an important question?\n",
    "<br>\n",
    "**But there comes another question ?**\n",
    "<br>\n",
    "Why we built k-skip n-grams? \n",
    "<br>\n",
    "Since we wanted to find out neighbourhood and contextual words coressponding to every word. So, k-skip n-gram created that for us here and with the help of n-gram here we want do define context of how many words do we want to give to our model. So if you are building 3 grams then you will be giving context of 3 neighbourhood words to your model.\n",
    "#### Ok now we know which words are neighbours. Now what?\n",
    "Once you have built k-skip n-gram model you would want to increase the probability of occuring of the given word with it's neighbourhood words. So in order to do that you would like to increase the probabilities of neighbourhood words(predict neighbourhood words) out of all the words in the output layer **but how will you do that?**\n",
    "In order to do that you'd try to adjust the weights in hidden layer in such a way that it increases the probability of the neighbourhood word in the output layer. For this purpose you'd use different optimisation algorithms like in this case **gradient descent** will be used which will help in adjusting the weights of hidden layer.\n",
    "<br>\n",
    "So now you have prepared the model which will have high probabilities for the neighbourhood words but that is not the only purpose of a word2vec model. \n",
    "<br>\n",
    "\n",
    "Since hidden layer will have vectors of every word but if we need vector for a word we will multiply hidden layer matrix with one-hot encoding of that word.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Conclusion :** So now using Word2vec we can reduce textual data in very high dimension to 100, 200 or 300 dimensions(as per the model requirement) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's implement word2vec with H2O "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Libraries read operation\n",
    "**Importing required libraries, like `h2o`, `pandas`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "; Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode)\n",
      "  Starting server from C:\\Users\\paras.mal\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
      "  Ice root: C:\\Users\\paras.mal\\AppData\\Local\\Temp\\tmpfw6mvw75\n",
      "  JVM stdout: C:\\Users\\paras.mal\\AppData\\Local\\Temp\\tmpfw6mvw75\\h2o_paras_mal_started_from_python.out\n",
      "  JVM stderr: C:\\Users\\paras.mal\\AppData\\Local\\Temp\\tmpfw6mvw75\\h2o_paras_mal_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321... successful.\n",
      "Warning: Your H2O cluster version is too old (10 months and 26 days)! Please download and install the latest version from http://h2o.ai/download/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>06 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Asia/Kolkata</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.18.0.8</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>10 months and 26 days !!!</td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_paras_mal_jtmem5</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>3.531 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.6 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  --------------------------------\n",
       "H2O cluster uptime:         06 secs\n",
       "H2O cluster timezone:       Asia/Kolkata\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.18.0.8\n",
       "H2O cluster version age:    10 months and 26 days !!!\n",
       "H2O cluster name:           H2O_from_python_paras_mal_jtmem5\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    3.531 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.6 final\n",
       "--------------------------  --------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h2o\n",
    "h2o.init()\n",
    "from IPython.core.display import Image, display\n",
    "from h2o.estimators.word2vec import H2OWord2vecEstimator\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data `preprocessing` step :\n",
    "- Reading csv file \n",
    "- Removing b' from each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>849636868052275200</td>\n",
       "      <td>2017-04-05 14:56:29</td>\n",
       "      <td>b'And so the robots spared humanity ... https:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>848988730585096192</td>\n",
       "      <td>2017-04-03 20:01:01</td>\n",
       "      <td>b\"@ForIn2020 @waltmossberg @mims @defcon_5 Exa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>848943072423497728</td>\n",
       "      <td>2017-04-03 16:59:35</td>\n",
       "      <td>b'@waltmossberg @mims @defcon_5 Et tu, Walt?'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>848935705057280001</td>\n",
       "      <td>2017-04-03 16:30:19</td>\n",
       "      <td>b'Stormy weather in Shortville ...'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>848416049573658624</td>\n",
       "      <td>2017-04-02 06:05:23</td>\n",
       "      <td>b\"@DaveLeeBBC @verge Coal is dying due to nat ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id           created_at  \\\n",
       "0  849636868052275200  2017-04-05 14:56:29   \n",
       "1  848988730585096192  2017-04-03 20:01:01   \n",
       "2  848943072423497728  2017-04-03 16:59:35   \n",
       "3  848935705057280001  2017-04-03 16:30:19   \n",
       "4  848416049573658624  2017-04-02 06:05:23   \n",
       "\n",
       "                                                text  \n",
       "0  b'And so the robots spared humanity ... https:...  \n",
       "1  b\"@ForIn2020 @waltmossberg @mims @defcon_5 Exa...  \n",
       "2      b'@waltmossberg @mims @defcon_5 Et tu, Walt?'  \n",
       "3                b'Stormy weather in Shortville ...'  \n",
       "4  b\"@DaveLeeBBC @verge Coal is dying due to nat ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"elonmusk_tweets.csv\") \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>849636868052275200</td>\n",
       "      <td>2017-04-05 14:56:29</td>\n",
       "      <td>And so the robots spared humanity ... https://...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>848988730585096192</td>\n",
       "      <td>2017-04-03 20:01:01</td>\n",
       "      <td>@ForIn2020 @waltmossberg @mims @defcon_5 Exact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>848943072423497728</td>\n",
       "      <td>2017-04-03 16:59:35</td>\n",
       "      <td>@waltmossberg @mims @defcon_5 Et tu, Walt?'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>848935705057280001</td>\n",
       "      <td>2017-04-03 16:30:19</td>\n",
       "      <td>Stormy weather in Shortville ...'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>848416049573658624</td>\n",
       "      <td>2017-04-02 06:05:23</td>\n",
       "      <td>@DaveLeeBBC @verge Coal is dying due to nat ga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id           created_at  \\\n",
       "0  849636868052275200  2017-04-05 14:56:29   \n",
       "1  848988730585096192  2017-04-03 20:01:01   \n",
       "2  848943072423497728  2017-04-03 16:59:35   \n",
       "3  848935705057280001  2017-04-03 16:30:19   \n",
       "4  848416049573658624  2017-04-02 06:05:23   \n",
       "\n",
       "                                                text  \n",
       "0  And so the robots spared humanity ... https://...  \n",
       "1  @ForIn2020 @waltmossberg @mims @defcon_5 Exact...  \n",
       "2        @waltmossberg @mims @defcon_5 Et tu, Walt?'  \n",
       "3                  Stormy weather in Shortville ...'  \n",
       "4  @DaveLeeBBC @verge Coal is dying due to nat ga...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(lambda x : str(x)[2:])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Converting dataframe to `h2o` frame \n",
    "- Column type of text column is strings\n",
    "- As H2o `Word2Vec` accepts string columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paras.mal\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\h2o\\utils\\shared_utils.py:170: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  data = _handle_python_lists(python_obj.as_matrix().tolist(), -1)[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "temp = h2o.H2OFrame(df[['text']], column_types=['string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2819, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning process \n",
    "- Tokenize the sentence into words\n",
    "- Convert all words into lower case\n",
    "- Filter the words which has only single charchter\n",
    "- Remove the `Numbers` from tweets\n",
    "- Remove `stop-words` from tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STOP_WORDS = [\"ax\",\"i\",\"you\",\"edu\",\"s\",\"t\",\"m\",\"subject\",\"can\",\"lines\",\"re\",\"what\",\"hi\",\n",
    "               \"there\",\"all\",\"we\",\"one\",\"the\",\"a\",\"an\",\"of\",\"or\",\"in\",\"for\",\"by\",\"on\",\n",
    "               \"but\",\"is\",\"in\",\"a\",\"not\",\"with\",\"as\",\"was\",\"if\",\"they\",\"are\",\"this\",\"and\",\"it\",\"have\",\n",
    "               \"from\",\"at\",\"my\",\"be\",\"by\",\"not\",\"that\",\"to\",\"from\",\"com\",\"org\",\"like\",\"likes\",\"so\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sentences, stop_word = STOP_WORDS):\n",
    "    tokenized = sentences.tokenize(\"\\\\W+\")\n",
    "    tokenized_lower = tokenized.tolower()\n",
    "    tokenized_filtered = tokenized_lower[(tokenized_lower.nchar() >= 2) | (tokenized_lower.isna()),:]\n",
    "    tokenized_words = tokenized_filtered[tokenized_filtered.grep(\"[0-9]\",invert=True,output_logical=True),:]\n",
    "    tokenized_words = tokenized_words[(tokenized_words.isna()) | (~ tokenized_words.isin(STOP_WORDS)),:]\n",
    "    return tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Break Notes into sequence of words\n"
     ]
    }
   ],
   "source": [
    "print(\"Break Notes into sequence of words\")\n",
    "words = tokenize(temp['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>C1          </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>robots      </td></tr>\n",
       "<tr><td>spared      </td></tr>\n",
       "<tr><td>humanity    </td></tr>\n",
       "<tr><td>https       </td></tr>\n",
       "<tr><td>co          </td></tr>\n",
       "<tr><td>            </td></tr>\n",
       "<tr><td>waltmossberg</td></tr>\n",
       "<tr><td>mims        </td></tr>\n",
       "<tr><td>exactly     </td></tr>\n",
       "<tr><td>tesla       </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33368, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training h2o `word2vec`\n",
    "- 3 hyperparameters have been used here.\n",
    "- `Sent_sample_rate` is used to downsample high frequency words\n",
    "- `Epoch parameter` specifies how many training iterations should be ran\n",
    "- `Vec_size` parameter tells that in how many dimension vectors coresponding to every word will be distributed\n",
    "- Finally the model is saved in desired location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Word2vec Model\n",
      "word2vec Model Build progress: |██████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\Personal Project\\\\word2vec\\\\Model\\\\Word2Vec_model_python_1552847583187_1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Build Word2vec Model\")\n",
    "w2v_model = H2OWord2vecEstimator(sent_sample_rate = 0.0, epochs = 15,vec_size = 100)\n",
    "w2v_model.train(training_frame=words['C1'])\n",
    "h2o.save_model(model=w2v_model, path=\"Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> Using find_synonyms we can find words appearing close to the word in `higher dimensional` space\n",
    "- the resultant dictionary will be an ordered dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('life', 0.7273121476173401),\n",
       "             ('survival', 0.6959032416343689),\n",
       "             ('exciting', 0.677527129650116),\n",
       "             ('them', 0.6674196720123291),\n",
       "             ('future', 0.662167489528656)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.find_synonyms(\"humanity\", count = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Now we will use word2vec model to create vectors coresponding to each word in 100-D space**\n",
    "- aggregate_method: Specifies how to aggregate sequences of words. If the method is NONE, then no aggregation is performed, and each input word is mapped to a single word-vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "vec = w2v_model.transform(words, aggregate_method = \"AVERAGE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Below is 100-D vector for each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">       C1</th><th style=\"text-align: right;\">      C2</th><th style=\"text-align: right;\">       C3</th><th style=\"text-align: right;\">         C4</th><th style=\"text-align: right;\">        C5</th><th style=\"text-align: right;\">       C6</th><th style=\"text-align: right;\">       C7</th><th style=\"text-align: right;\">       C8</th><th style=\"text-align: right;\">        C9</th><th style=\"text-align: right;\">      C10</th><th style=\"text-align: right;\">      C11</th><th style=\"text-align: right;\">      C12</th><th style=\"text-align: right;\">      C13</th><th style=\"text-align: right;\">       C14</th><th style=\"text-align: right;\">      C15</th><th style=\"text-align: right;\">        C16</th><th style=\"text-align: right;\">      C17</th><th style=\"text-align: right;\">      C18</th><th style=\"text-align: right;\">      C19</th><th style=\"text-align: right;\">        C20</th><th style=\"text-align: right;\">      C21</th><th style=\"text-align: right;\">       C22</th><th style=\"text-align: right;\">       C23</th><th style=\"text-align: right;\">      C24</th><th style=\"text-align: right;\">     C25</th><th style=\"text-align: right;\">        C26</th><th style=\"text-align: right;\">      C27</th><th style=\"text-align: right;\">       C28</th><th style=\"text-align: right;\">      C29</th><th style=\"text-align: right;\">      C30</th><th style=\"text-align: right;\">        C31</th><th style=\"text-align: right;\">       C32</th><th style=\"text-align: right;\">       C33</th><th style=\"text-align: right;\">       C34</th><th style=\"text-align: right;\">        C35</th><th style=\"text-align: right;\">      C36</th><th style=\"text-align: right;\">       C37</th><th style=\"text-align: right;\">       C38</th><th style=\"text-align: right;\">     C39</th><th style=\"text-align: right;\">       C40</th><th style=\"text-align: right;\">       C41</th><th style=\"text-align: right;\">      C42</th><th style=\"text-align: right;\">      C43</th><th style=\"text-align: right;\">      C44</th><th style=\"text-align: right;\">      C45</th><th style=\"text-align: right;\">       C46</th><th style=\"text-align: right;\">      C47</th><th style=\"text-align: right;\">      C48</th><th style=\"text-align: right;\">      C49</th><th style=\"text-align: right;\">        C50</th><th style=\"text-align: right;\">      C51</th><th style=\"text-align: right;\">      C52</th><th style=\"text-align: right;\">      C53</th><th style=\"text-align: right;\">      C54</th><th style=\"text-align: right;\">     C55</th><th style=\"text-align: right;\">      C56</th><th style=\"text-align: right;\">      C57</th><th style=\"text-align: right;\">      C58</th><th style=\"text-align: right;\">       C59</th><th style=\"text-align: right;\">     C60</th><th style=\"text-align: right;\">      C61</th><th style=\"text-align: right;\">      C62</th><th style=\"text-align: right;\">      C63</th><th style=\"text-align: right;\">      C64</th><th style=\"text-align: right;\">      C65</th><th style=\"text-align: right;\">      C66</th><th style=\"text-align: right;\">       C67</th><th style=\"text-align: right;\">     C68</th><th style=\"text-align: right;\">     C69</th><th style=\"text-align: right;\">       C70</th><th style=\"text-align: right;\">       C71</th><th style=\"text-align: right;\">     C72</th><th style=\"text-align: right;\">       C73</th><th style=\"text-align: right;\">       C74</th><th style=\"text-align: right;\">       C75</th><th style=\"text-align: right;\">       C76</th><th style=\"text-align: right;\">     C77</th><th style=\"text-align: right;\">      C78</th><th style=\"text-align: right;\">       C79</th><th style=\"text-align: right;\">      C80</th><th style=\"text-align: right;\">      C81</th><th style=\"text-align: right;\">     C82</th><th style=\"text-align: right;\">      C83</th><th style=\"text-align: right;\">       C84</th><th style=\"text-align: right;\">      C85</th><th style=\"text-align: right;\">       C86</th><th style=\"text-align: right;\">     C87</th><th style=\"text-align: right;\">        C88</th><th style=\"text-align: right;\">     C89</th><th style=\"text-align: right;\">       C90</th><th style=\"text-align: right;\">       C91</th><th style=\"text-align: right;\">      C92</th><th style=\"text-align: right;\">     C93</th><th style=\"text-align: right;\">       C94</th><th style=\"text-align: right;\">     C95</th><th style=\"text-align: right;\">     C96</th><th style=\"text-align: right;\">     C97</th><th style=\"text-align: right;\">       C98</th><th style=\"text-align: right;\">      C99</th><th style=\"text-align: right;\">     C100</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">-0.140124</td><td style=\"text-align: right;\">0.115578</td><td style=\"text-align: right;\">0.137378 </td><td style=\"text-align: right;\">-0.192654  </td><td style=\"text-align: right;\">-0.142361 </td><td style=\"text-align: right;\">-0.222518</td><td style=\"text-align: right;\">-0.212257</td><td style=\"text-align: right;\">0.0944436</td><td style=\"text-align: right;\">-0.0500358</td><td style=\"text-align: right;\">0.0408906</td><td style=\"text-align: right;\">0.125791 </td><td style=\"text-align: right;\">0.0715994</td><td style=\"text-align: right;\">-0.24141 </td><td style=\"text-align: right;\">-0.0363606</td><td style=\"text-align: right;\">0.0747646</td><td style=\"text-align: right;\">-0.087048  </td><td style=\"text-align: right;\">0.0961847</td><td style=\"text-align: right;\">0.318623 </td><td style=\"text-align: right;\">0.127406 </td><td style=\"text-align: right;\">-0.00814109</td><td style=\"text-align: right;\">0.183734 </td><td style=\"text-align: right;\">-0.190772 </td><td style=\"text-align: right;\">-0.197582 </td><td style=\"text-align: right;\">0.0558142</td><td style=\"text-align: right;\">0.268359</td><td style=\"text-align: right;\">0.000782574</td><td style=\"text-align: right;\">-0.154784</td><td style=\"text-align: right;\"> 0.0138605</td><td style=\"text-align: right;\">0.199373 </td><td style=\"text-align: right;\">0.0324799</td><td style=\"text-align: right;\">-0.00671948</td><td style=\"text-align: right;\">-0.0164448</td><td style=\"text-align: right;\">-0.0166572</td><td style=\"text-align: right;\">-0.0428746</td><td style=\"text-align: right;\">-0.0825788 </td><td style=\"text-align: right;\">0.104213 </td><td style=\"text-align: right;\">-0.0518572</td><td style=\"text-align: right;\"> 0.109531 </td><td style=\"text-align: right;\">0.198774</td><td style=\"text-align: right;\">-0.118415 </td><td style=\"text-align: right;\">0.00451641</td><td style=\"text-align: right;\">0.131714 </td><td style=\"text-align: right;\">0.0301582</td><td style=\"text-align: right;\">-0.17077 </td><td style=\"text-align: right;\">-0.232667</td><td style=\"text-align: right;\"> 0.0119375</td><td style=\"text-align: right;\">0.107875 </td><td style=\"text-align: right;\">0.0990394</td><td style=\"text-align: right;\">0.172051 </td><td style=\"text-align: right;\"> 0.00331667</td><td style=\"text-align: right;\">0.259763 </td><td style=\"text-align: right;\">-0.166099</td><td style=\"text-align: right;\">0.0267812</td><td style=\"text-align: right;\">-0.234313</td><td style=\"text-align: right;\">0.220877</td><td style=\"text-align: right;\">-0.203211</td><td style=\"text-align: right;\">0.158323 </td><td style=\"text-align: right;\">0.137339 </td><td style=\"text-align: right;\">0.116616  </td><td style=\"text-align: right;\">0.193931</td><td style=\"text-align: right;\">-0.109274</td><td style=\"text-align: right;\">0.10889  </td><td style=\"text-align: right;\">0.0208958</td><td style=\"text-align: right;\">0.197201 </td><td style=\"text-align: right;\">0.0186227</td><td style=\"text-align: right;\">0.0834158</td><td style=\"text-align: right;\">-0.18026  </td><td style=\"text-align: right;\">0.152136</td><td style=\"text-align: right;\">0.270947</td><td style=\"text-align: right;\">-0.0115342</td><td style=\"text-align: right;\">0.10143   </td><td style=\"text-align: right;\">0.187187</td><td style=\"text-align: right;\">-0.0813169</td><td style=\"text-align: right;\">-0.137847 </td><td style=\"text-align: right;\">-0.0931884</td><td style=\"text-align: right;\">-0.0441091</td><td style=\"text-align: right;\">0.185116</td><td style=\"text-align: right;\">-0.105353</td><td style=\"text-align: right;\"> 0.0485673</td><td style=\"text-align: right;\">-0.177656</td><td style=\"text-align: right;\">0.109172 </td><td style=\"text-align: right;\">0.179267</td><td style=\"text-align: right;\">0.19912  </td><td style=\"text-align: right;\">-0.130298 </td><td style=\"text-align: right;\">0.0919077</td><td style=\"text-align: right;\">-0.245947 </td><td style=\"text-align: right;\">0.165589</td><td style=\"text-align: right;\"> 0.1208    </td><td style=\"text-align: right;\">0.090146</td><td style=\"text-align: right;\"> 0.0141273</td><td style=\"text-align: right;\">-0.0353326</td><td style=\"text-align: right;\">-0.292456</td><td style=\"text-align: right;\">0.107669</td><td style=\"text-align: right;\">-0.0418254</td><td style=\"text-align: right;\">0.226459</td><td style=\"text-align: right;\">0.198535</td><td style=\"text-align: right;\">0.161463</td><td style=\"text-align: right;\">-0.0135548</td><td style=\"text-align: right;\">0.0811656</td><td style=\"text-align: right;\">-0.304476</td></tr>\n",
       "<tr><td style=\"text-align: right;\">-0.16967 </td><td style=\"text-align: right;\">0.206854</td><td style=\"text-align: right;\">0.0421439</td><td style=\"text-align: right;\">-0.00396038</td><td style=\"text-align: right;\">-0.0131857</td><td style=\"text-align: right;\">-0.115076</td><td style=\"text-align: right;\">-0.190208</td><td style=\"text-align: right;\">0.0758914</td><td style=\"text-align: right;\">-0.11229  </td><td style=\"text-align: right;\">0.14289  </td><td style=\"text-align: right;\">0.0355645</td><td style=\"text-align: right;\">0.0902681</td><td style=\"text-align: right;\">-0.178689</td><td style=\"text-align: right;\"> 0.0229995</td><td style=\"text-align: right;\">0.0934682</td><td style=\"text-align: right;\"> 0.00935052</td><td style=\"text-align: right;\">0.129002 </td><td style=\"text-align: right;\">0.0558293</td><td style=\"text-align: right;\">0.0428345</td><td style=\"text-align: right;\"> 0.0288177 </td><td style=\"text-align: right;\">0.0267488</td><td style=\"text-align: right;\">-0.0828091</td><td style=\"text-align: right;\">-0.0788152</td><td style=\"text-align: right;\">0.0619851</td><td style=\"text-align: right;\">0.146143</td><td style=\"text-align: right;\">0.0478584  </td><td style=\"text-align: right;\">-0.179795</td><td style=\"text-align: right;\">-0.0174663</td><td style=\"text-align: right;\">0.0743986</td><td style=\"text-align: right;\">0.0233767</td><td style=\"text-align: right;\">-0.0126196 </td><td style=\"text-align: right;\">-0.0670774</td><td style=\"text-align: right;\">-0.0221247</td><td style=\"text-align: right;\">-0.0282587</td><td style=\"text-align: right;\"> 0.00912785</td><td style=\"text-align: right;\">0.0235637</td><td style=\"text-align: right;\"> 0.0228713</td><td style=\"text-align: right;\">-0.0478296</td><td style=\"text-align: right;\">0.144907</td><td style=\"text-align: right;\"> 0.0681545</td><td style=\"text-align: right;\">0.0351881 </td><td style=\"text-align: right;\">0.0359154</td><td style=\"text-align: right;\">0.0262917</td><td style=\"text-align: right;\">-0.064895</td><td style=\"text-align: right;\">-0.178923</td><td style=\"text-align: right;\">-0.148187 </td><td style=\"text-align: right;\">0.0323386</td><td style=\"text-align: right;\">0.084935 </td><td style=\"text-align: right;\">0.0677829</td><td style=\"text-align: right;\">-0.0490558 </td><td style=\"text-align: right;\">0.0490368</td><td style=\"text-align: right;\">-0.145193</td><td style=\"text-align: right;\">0.0257258</td><td style=\"text-align: right;\">-0.140599</td><td style=\"text-align: right;\">0.111788</td><td style=\"text-align: right;\">-0.17087 </td><td style=\"text-align: right;\">0.0740502</td><td style=\"text-align: right;\">0.0879119</td><td style=\"text-align: right;\">0.00678276</td><td style=\"text-align: right;\">0.140466</td><td style=\"text-align: right;\">-0.129329</td><td style=\"text-align: right;\">0.0556249</td><td style=\"text-align: right;\">0.0719198</td><td style=\"text-align: right;\">0.0881176</td><td style=\"text-align: right;\">0.0513157</td><td style=\"text-align: right;\">0.0352245</td><td style=\"text-align: right;\">-0.0678333</td><td style=\"text-align: right;\">0.131806</td><td style=\"text-align: right;\">0.11959 </td><td style=\"text-align: right;\"> 0.0148897</td><td style=\"text-align: right;\">0.00791822</td><td style=\"text-align: right;\">0.182454</td><td style=\"text-align: right;\">-0.0761149</td><td style=\"text-align: right;\">-0.0855969</td><td style=\"text-align: right;\">-0.0708252</td><td style=\"text-align: right;\">-0.0503814</td><td style=\"text-align: right;\">0.241522</td><td style=\"text-align: right;\">-0.100864</td><td style=\"text-align: right;\">-0.0274989</td><td style=\"text-align: right;\">-0.171782</td><td style=\"text-align: right;\">0.0719592</td><td style=\"text-align: right;\">0.135548</td><td style=\"text-align: right;\">0.0789291</td><td style=\"text-align: right;\">-0.0341286</td><td style=\"text-align: right;\">0.0393357</td><td style=\"text-align: right;\">-0.0596421</td><td style=\"text-align: right;\">0.152296</td><td style=\"text-align: right;\">-0.00441801</td><td style=\"text-align: right;\">0.016978</td><td style=\"text-align: right;\">-0.0667887</td><td style=\"text-align: right;\">-0.0230533</td><td style=\"text-align: right;\">-0.138561</td><td style=\"text-align: right;\">0.137034</td><td style=\"text-align: right;\">-0.0305867</td><td style=\"text-align: right;\">0.147919</td><td style=\"text-align: right;\">0.195176</td><td style=\"text-align: right;\">0.28544 </td><td style=\"text-align: right;\"> 0.0863056</td><td style=\"text-align: right;\">0.159609 </td><td style=\"text-align: right;\">-0.14301 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2819, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Saving word2vec vectors and mapping tweets to be used for different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export File progress: |███████████████████████████████████████████████████| 100%\n",
      "Export File progress: |███████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "h2o.export_file(vec,'w2v_vectors_lat.csv')\n",
    "h2o.export_file(temp,'w2v_words_lat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# To be continued ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
